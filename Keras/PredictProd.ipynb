{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read from file  DJI\n",
      "Read from file  CVX\n",
      "Loaded model  CVX  from disk\n",
      "Finished MakePredictions\n",
      " it predicted  35 out of 64  dates in last 3 years,and  25 ( 0.71 %) were right\n",
      "Read from file  DIS\n",
      "Loaded model  DIS  from disk\n",
      "Finished MakePredictions\n",
      " it predicted  14 out of 37  dates in last 3 years,and  14 ( 1.00 %) were right\n",
      "Read from file  DWDP\n",
      "Loaded model  DWDP  from disk\n",
      "Finished MakePredictions\n",
      " it predicted  50 out of 69  dates in last 3 years,and  40 ( 0.80 %) were right\n",
      "Read from file  GE\n",
      "Loaded model  GE  from disk\n",
      "Finished MakePredictions\n",
      " it predicted  22 out of 36  dates in last 3 years,and  21 ( 0.95 %) were right\n",
      "Read from file  GS\n",
      "Loaded model  GS  from disk\n",
      "Finished MakePredictions\n",
      " it predicted  67 out of 77  dates in last 3 years,and  53 ( 0.79 %) were right\n",
      "Read from file  HD\n",
      "Loaded model  HD  from disk\n",
      "Finished MakePredictions\n",
      " it predicted  29 out of 43  dates in last 3 years,and  26 ( 0.90 %) were right\n",
      "----------got it HD\n",
      "Read from file  IBM\n",
      "Loaded model  IBM  from disk\n",
      "Finished MakePredictions\n",
      " it predicted  9 out of 37  dates in last 3 years,and  8 ( 0.89 %) were right\n",
      "Read from file  INTC\n",
      "Loaded model  INTC  from disk\n",
      "Finished MakePredictions\n",
      " it predicted  51 out of 67  dates in last 3 years,and  39 ( 0.76 %) were right\n",
      "Read from file  JNJ\n",
      "Loaded model  JNJ  from disk\n",
      "Finished MakePredictions\n",
      " it predicted  7 out of 14  dates in last 3 years,and  3 ( 0.43 %) were right\n",
      "----------got it JNJ\n",
      "Read from file  JPM\n",
      "Loaded model  JPM  from disk\n",
      "Finished MakePredictions\n",
      " it predicted  35 out of 59  dates in last 3 years,and  32 ( 0.91 %) were right\n",
      "Read from file  KO\n",
      "Loaded model  KO  from disk\n",
      "Finished MakePredictions\n",
      " it predicted  0 out of 2  dates in last 3 years,and  0 ( 0 %) were right\n",
      "Read from file  MCD\n",
      "Loaded model  MCD  from disk\n",
      "Finished MakePredictions\n",
      " it predicted  21 out of 21  dates in last 3 years,and  17 ( 0.81 %) were right\n",
      "Read from file  MMM\n",
      "Loaded model  MMM  from disk\n",
      "Finished MakePredictions\n",
      " it predicted  10 out of 28  dates in last 3 years,and  10 ( 1.00 %) were right\n",
      "Read from file  MRK\n",
      "Loaded model  MRK  from disk\n",
      "Finished MakePredictions\n",
      " it predicted  9 out of 35  dates in last 3 years,and  9 ( 1.00 %) were right\n",
      "Read from file  MSFT\n",
      "Loaded model  MSFT  from disk\n",
      "Finished MakePredictions\n",
      " it predicted  29 out of 50  dates in last 3 years,and  28 ( 0.97 %) were right\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas_datareader import data as web\n",
    "from pandas import read_csv\n",
    "from pandas import concat\n",
    "import os.path\n",
    "import datetime \n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import numpy\n",
    "\n",
    "mypath='C:\\\\Users\\\\mwnuk\\\\Dropbox\\\\Quotes\\\\'\n",
    "myModelpath='C:\\\\Users\\\\mwnuk\\\\Dropbox\\\\Models\\\\'\n",
    "settingsPath='C:\\\\Users\\\\mwnuk\\\\Dropbox\\\\Settings\\\\'\n",
    "reportPath='C:\\\\Users\\\\mwnuk\\\\Dropbox\\\\Reports\\\\'\n",
    "\n",
    "# Stochastic Oscilator %K\n",
    "def STOK(close, low, high, n): \n",
    " STOK = ((close - pd.Series.rolling(low, n).min()) / (pd.Series.rolling(high, n).max() - pd.Series.rolling(low, n).min())) * 100\n",
    " return STOK\n",
    "\n",
    "# Stochastic Oscilator %D\n",
    "def STOD(close, low, high, n):\n",
    "    STOK = ((close - pd.Series.rolling(low, n).min()) / (pd.Series.rolling(high, n).max() - pd.Series.rolling(low, n).min())) * 100\n",
    "    STOD = pd.Series.rolling(STOK, 3).mean()\n",
    "    return STOD\n",
    "    #STOD\n",
    "\n",
    "    \n",
    "def RSI(series, period):\n",
    "    delta = series.diff().dropna()\n",
    "    u = delta * 0\n",
    "    d = u.copy()\n",
    "    u[delta > 0] = delta[delta > 0]\n",
    "    d[delta < 0] = -delta[delta < 0]\n",
    "    u[u.index[period-1]] = np.mean( u[:period] ) #first value is sum of avg gains\n",
    "    u = u.drop(u.index[:(period-1)])\n",
    "    d[d.index[period-1]] = np.mean( d[:period] ) #first value is sum of avg losses\n",
    "    d = d.drop(d.index[:(period-1)])\n",
    "   # rs = pd.stats.moments.ewma(u, com=period-1, adjust=False) / \\\n",
    "   #      pd.stats.moments.ewma(d, com=period-1, adjust=False)\n",
    "    rs= pd.Series.ewm(u,com=period-1, min_periods=0,adjust=False,ignore_na=False).mean() / \\\n",
    "        pd.Series.ewm(d,com=period-1,min_periods=0,adjust=False,ignore_na=False).mean()\n",
    "    return 100 - 100 / (1 + rs)    \n",
    "\n",
    "\n",
    "def getData(item):\n",
    "    start = datetime.datetime(2016, 12, 21)\n",
    "    end = datetime.datetime.now()\n",
    "    #print( start,end) \n",
    "    file_path=mypath+item +'.csv'\n",
    "    if not os.path.exists(file_path):\n",
    "        print(\"No data\")\n",
    "    else:\n",
    "        print(\"Read from file \", item)  \n",
    "        try:     \n",
    "            df = read_csv(file_path)\n",
    "            df['Date']=pd.to_datetime(df['Date']) #important for sorting\n",
    "            df.set_index(\"Date\",inplace=True)\n",
    "        except: print(\"Exception caught\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def cleanData( df):\n",
    "    # drop Close and Volume, keep Adj Close\n",
    "    \n",
    "    df1 = df.drop('Close', 1)\n",
    "    df1 = df1.drop('Volume', 1)\n",
    "    if 'y' in df1.columns:\n",
    "        df1 = df1.drop('y', 1)\n",
    "    if 'yhat' in df1.columns:\n",
    "        df1 = df1.drop('yhat', 1)\n",
    "    if 'yhathat' in df1.columns:\n",
    "        df1 = df1.drop('yhathat', 1)\n",
    "    df1.rename(columns={'Adj Close': 'Close', 'oldName2': 'newName2'}, inplace=True)\n",
    "    return df1\n",
    "\n",
    "\n",
    "def engFeatures(df):\n",
    "    df['%K'] = STOK(df['Close'], df['Low'], df['High'], 14)\n",
    "    df['%D'] = STOD(df['Close'], df['Low'], df['High'], 14)\n",
    "    df['Avg5'] =pd.Series.rolling(df['Close'],5).mean()\n",
    "    df['Avg10'] =pd.Series.rolling(df['Close'],10).mean()\n",
    "    df['RSI14'] = RSI(df['Close'],14)\n",
    "    df['RSI7'] = RSI(df['Close'],7)\n",
    "    #df['Date']=pd.to_datetime(df['Date'])\n",
    "    df.sort_index(ascending=False,inplace=True)\n",
    "    df['Rise'] = (  pd.Series.rolling(df['Close'],5).max()-df['Close'] >df['Close']*0.04)*1\n",
    "    df.sort_index(ascending=True,inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "# convert series to supervised learning\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg\n",
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def rescale( df):\n",
    "    df.dropna(inplace=True)\n",
    "    values = df.values\n",
    "    # ensure all data is float\n",
    "    values = values.astype('float')\n",
    "\n",
    "    timesteps=3\n",
    "\n",
    "    features = values.shape[1]-1\n",
    "    # normalize features\n",
    "    scaler = MinMaxScaler()\n",
    "    #scaled = scaler.fit_transform(values)\n",
    "    scaled = values #scaler.fit_transform(values)\n",
    "    # frame as supervised learning\n",
    "    reframed = series_to_supervised(scaled,timesteps, 1)\n",
    "    # drop columns we don't want to predict\n",
    "    #reframed.drop(reframed.columns[[10,11,12,13,14,15,16,17,18,19,20]], axis=1, inplace=True)\n",
    "    reframed= pd.DataFrame(reframed)\n",
    "    return reframed\n",
    "#print(reframed.head())\n",
    "#reframed.iloc[0]\n",
    "\n",
    "#def cleadData( df1,df2):\n",
    "    \n",
    "#    return cleanSet\n",
    "#####################################################################3\n",
    "def loadModel( item ):\n",
    "    # later...\n",
    "    # load json and create model\n",
    "    from keras.models import model_from_json\n",
    "\n",
    "    json_file = open(myModelpath + item+'_model.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights(myModelpath + item +'_model.h5')\n",
    "    print(\"Loaded model \", item,\" from disk\")\n",
    "    # evaluate loaded model on test data\n",
    "    #loaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy']) #61.29\n",
    "    loaded_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    #score = loaded_model.evaluate( test_X, test_y, verbose=0)\n",
    "    #print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))\n",
    "    return loaded_model\n",
    "\n",
    "#######################################################################\n",
    "def MakePredictions( dfr):\n",
    "    \n",
    "    dframeclean=cleanData(dfr)\n",
    "    #djiframe=getData('DJI')  # this can be loaded once\n",
    "    #djiframe=cleanData(djiframe)\n",
    "    #dframe=dframe.join(djiframe, lsuffix='_left', rsuffix='_right')\n",
    "    dframeclean=dframeclean.join(djiframe , rsuffix='_right')\n",
    "    fullFrame=engFeatures(dframeclean)\n",
    "    reframed=rescale(fullFrame)\n",
    "    \n",
    "    #test on entire dataframe\n",
    "    test = reframed.values\n",
    "    test_X, test_y = test[:, :-1], to_categorical(test[:, -1])\n",
    "    test_X = numpy.reshape(test_X, (test_X.shape[0], test_X.shape[1],1))\n",
    "    #print(test_X.shape)\n",
    "    yhat = jason_model.predict(test_X)\n",
    "    from numpy import argmax\n",
    "    yhat=argmax(yhat,axis=1)\n",
    "    #print(test_X.shape, test_y.shape,yhat.shape, dframe.shape)\n",
    "   \n",
    "    # reframing makes original dframe shorter\n",
    "    # below code makes them all equal length by padding with zeros\n",
    "\n",
    "    yhat=yhat.reshape(yhat.shape[0],1)\n",
    "    #print(dfr.shape,yhat.shape)\n",
    "    nz=pd.DataFrame(np.zeros(dfr.shape[0]-yhat.shape[0]))\n",
    "    yhat1=nz.append(pd.DataFrame(yhat) )\n",
    "    y=nz.append( pd.DataFrame(argmax(test_y,1) ))\n",
    "    #print( yhat1.shape, y.shape )\n",
    "    \n",
    "    #put results back into dframe\n",
    "    dfr['y']=y.values\n",
    "    dfr['yhat']=yhat1.values\n",
    "    dfr['yhathat']=np.zeros(yhat1.shape[0]) # prepare empty column\n",
    "    print(\"Finished MakePredictions\" )\n",
    "    return dfr\n",
    "#############################################################################\n",
    "#############################################################################\n",
    "## MAIN EXECUTION \n",
    "file = open(reportPath + 'Report.txt','w+') \n",
    "df1 = read_csv(settingsPath + 'List1.csv', header=None)\n",
    "df2 = read_csv(settingsPath + 'List2.csv', header=None)\n",
    "df3 = read_csv(settingsPath + 'List3.csv', header=None)\n",
    "df4 = read_csv(settingsPath + 'List4.csv', header=None)\n",
    "df5 = read_csv(settingsPath + 'List5.csv', header=None)\n",
    "df6 = read_csv(settingsPath + 'List6.csv', header=None)\n",
    "df7 = read_csv(settingsPath + 'BetaList.csv', header=None)\n",
    "#df6 = read_csv(settingsPath + 'List6 - test.csv', header=None)\n",
    "df=pd.concat([df2,df3,df4])\n",
    "#df=df1\n",
    "#df=pd.concat([df5,df6])\n",
    "#df=df7\n",
    "djiframe=getData('DJI')  # this can be loaded once\n",
    "djiframe=cleanData(djiframe)\n",
    "\n",
    "file.write(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") +  \" Verified \" +str(len(df.index)) + \"models \\n\") \n",
    "\n",
    "for index in range (df.shape[0]):\n",
    "    itemname=df.iloc[index,0]\n",
    "    #print ('Current item :', itemname)\n",
    "    #itemname = 'BA'\n",
    "    dframe = getData(itemname)\n",
    "    jason_model = loadModel(itemname)\n",
    "\n",
    "    df1  = MakePredictions(dframe)\n",
    "    \n",
    "#Analyse statistics\n",
    "    y_cnt=df1.index[df1['yhat'] >0].size\n",
    "    yhat_cnt = df1.index[df1['y'] >0].size\n",
    "    tp_cnt = df1.index[np.where((df1['yhat'] >0) & (df1['y'] >0))].size \n",
    "    if( y_cnt>0):\n",
    "        percent='%3.2f'%(tp_cnt/y_cnt)\n",
    "    else:\n",
    "        percent=0\n",
    "    print( \" it predicted \", y_cnt, \"out of\",yhat_cnt,\" dates in last 3 years,and \",tp_cnt,\"(\",percent,\"%) were right\")\n",
    "    \n",
    "    with open(reportPath + \"History.txt\", \"a\") as text_file:\n",
    "        text_file.write(\"%4s predicted %d out of %d events in last 3 years, and %d ( %s %%) were right \\n\" % (itemname,y_cnt,yhat_cnt,tp_cnt,percent)  ) \n",
    "    \n",
    "    df2=df1[-2:]\n",
    "    if (df2['yhat'].any() > 0):\n",
    "        print('----------got it',itemname)\n",
    "        file.write(\"got it \" + itemname +\"\\n\") \n",
    "\n",
    "file.write(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") + \"\\n\") \n",
    "file.close() \n",
    "\n",
    "#PrintReport(df1)\n",
    "#df1.tail\n",
    "\n",
    "## get all predicted values yhat and verify they are still valid when \n",
    "## at the end of the dataset. Cut dataframe in pieces and predict yhathat\n",
    "#predictedlist=df1.index[df1['yhat'] >0]\n",
    "#predictedlist[0]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "len(df.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename=mypath+\"AAPL.csv\"\n",
    "fo = open(filename, \"r\")\n",
    "fo.seek(-80, os.SEEK_END)\n",
    "line = fo.readlines()[-1]\n",
    "line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getLastLine(fname, maxLineLength=80):\n",
    "  fp=file(fname, \"rb\")\n",
    "  fp.seek(-maxLineLength-1, 2) # 2 means \"from the end of the file\"\n",
    "  return fp.readlines()[-1]\n",
    "\n",
    "filename=mypath+\"AAPL.csv\"\n",
    "getLastLine(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
